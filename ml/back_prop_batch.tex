\documentclass[14pt,letter,oneside]{article}
\usepackage{geometry}
\geometry{left=20mm,top=20mm}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{physics}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}\usepackage{graphicx}

\begin{document}
\title{Derivation of Batch Back Propagation }
\maketitle
	
	\section*{Variables:}
	\begin{list}{-}{}
		\item{$X_{ij}, \hspace{8pt} 0\leq i < N, \hspace{8pt} 0\leq j < K^{(0)}$} \\ 
		Inputs:  $N$ row of input samples with $K + 1$ features. The first feature is always 1 for bias.
		
		\item{ $W^{(l)}_{ij}, \hspace{8pt} 0\leq l < L, \hspace{8pt} 0 \leq i < K^{(l)}, \hspace{8pt} 0 \leq j < K^{(l+1)} $} \\
		Network Weights: Where $l$ is the layer in the network. $j$ is the number of neurons in that layer, $i$ is number of inputs from previous layer. Note $K_0 = K+1$ from the input.
		
		\item{ $A^l= Z^l W^l, \hspace{8pt} 0 \leq l < L$ } \\
		Activation of the $l$-th layer. Note that $Z^0 = X$
		
		\item { $Z^l = f^{l-1}(A^{l-1}), \hspace{8pt} 1 \leq l < L$} \\
		Output for layer $l$. 
	
		\item { $f^l(A^l) = f^l( Z^l W^l ) $} \\
		Filter function for layer $l$
		
		\item { $\tilde{Y}_{ij},  \hspace{8pt} 0\leq i < N, \hspace{8pt} 0\leq j < K^{(L)} $} \\
		Note $Y = Z^L$
		
		\item { $T_{ij}, \hspace{8pt} 0\leq i < N, \hspace{8pt} 0\leq j < K^{(L)} $ } \\
		Target value used to compared with Y
		
		\item { $E(\tilde{Y}, T) = E(Z^{(L)})$ } 
		\\ Error function 
 
	\end{list}
	 Note that $Z$ goes from $Z^{(0)}\ldots Z^{(L)}$, $f$ goes from $f^{(0)} \ldots f^{(L-1)}$, $A$ goes from $A^{(0)} \ldots A^{(L-1)}$ and $W$ goes from $W^{(0)} \ldots W^{(L-1)}$ \\\
	 
	 \section*{Derivation:}
	 For each layer $l$ by chain rule:
	 \begin{align}
	   \pdv{E}{W^{l}_{ij}} = \sum_{s,t} \pdv{E}{A^l_{st}} \pdv{A^l_{st}}{W^{l}_{ij}}
	 \end{align}
	 
	 Since \begin{align}
		A^l_{st} &= \sum_k Z^l_{sk}W^l_{kt} \\
		\pdv{A^l_{st}}{W^l_{ij}} &= Z^l_{sk} \delta^k_i \delta^t_j \\
		 &= Z^l_{si} \delta^t_j
	 \end{align} 
	 
	 Substitute (4) into (1), we have 
	 \begin{align}
		 \pdv{E}{W^{l}_{ij}} &= \sum_{s,t} \pdv{E}{A^l_{st}}  Z^l_{si} \delta^t_j \\
		 &= \sum_{s} \pdv{E}{A^l_{sj}}  Z^l_{si}
	 \end{align}
	 
	 
	 Also,
	 \begin{align}
		  \pdv{E}{A^l_{sj}} &= \sum_{u,v} \pdv{E}{A^{l+1}_{uv}} \pdv{A^{l+1}_{uv}}{A^l_{sj}} \\
		  &= \sum_{u,v} \pdv{E}{A^{l+1}_{uv}} \pdv{}{A^l_{sj}} \left(\sum_p \left[f^l(A^l)\right]_{up}W^{l+1}_{pv} \right)  \\
		  &= \sum_{u,v} \pdv{E}{A^{l+1}_{uv}} \left(\sum_p \left[\partial_Af^l(A^l)\right]_{up} \delta^u_s \delta^p_j W^{l+1}_{pv} \right)  \\
		 &= \sum_{u,v} \pdv{E}{A^{l+1}_{uv}} \left(\left[\partial_Af^l(A^l)\right]_{uj} \delta^u_s W^{l+1}_{jv} \right)  \\
		 &= \sum_{v} \pdv{E}{A^{l+1}_{sv}} \left(\left[\partial_Af^l(A^l)\right]_{sj} W^{l+1}_{jv} \right) 
	 \end{align}
	 
	 In matrix form: \\

	 
	 For base case, we have:
	   \begin{align}
	   		X &= Z^0 \\
	   		Y &= Z^L = f^{L-1}(A^{L-1})  \\
	   		E(Y) &= E(Z^L) = E(f^{L-1}(A^{L-1})) \\
	   		\pdv{E}{A^{L-1}} &= \pdv{E(Y)}{Y}\partial_A f^{L-1}(A^{L-1}) 
	   \end{align}
	   
	 For recursive case ($0 \leq l \leq L-2$):
	   \begin{align}
	    	\pdv{E}{W^l} &= (Z^l)^T \pdv{E}{A^l} \\
	    	\pdv{E}{A^l} &= \pdv{E}{A^{l+1}} \left( \partial_Af^l(A^l) W^{l+1} \right)^T \\
	    	&= \pdv{E}{A^{l+1}} \left( \partial_Af^l(Z^lW^l) W^{l+1} \right)^T
	   \end{align}
	
	 To update $W$:
	   $$ W^l \leftarrow W^l - \eta_l \pdv{E}{W^l} $$
	
\end{document}